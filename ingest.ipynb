{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "from transformers import AutoTokenizer\n",
    "from langchain_ibm import ChatWatsonx, WatsonxLLM, WatsonxEmbeddings\n",
    "from langchain_text_splitters import MarkdownTextSplitter\n",
    "import os\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import (\n",
    "    AcceleratorDevice,\n",
    "    AcceleratorOptions,\n",
    "    PdfPipelineOptions,\n",
    ")\n",
    "from docling.document_converter import DocumentConverter, PdfFormatOption\n",
    "from utils import *\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "PAGE_BREAK = \"<!-- page break -->\"\n",
    "\n",
    "file_paths = [ file for file in os.listdir(\"data/climate_edu\") if file.endswith(\".pdf\") ]\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"ibm-granite/granite-embedding-278m-multilingual\"\n",
    ")\n",
    "max_length = tokenizer.model_max_length - 10\n",
    "\n",
    "markdown_splitter = MarkdownTextSplitter.from_huggingface_tokenizer(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=max_length-10,\n",
    "    chunk_overlap=int(max_length/10)\n",
    ")\n",
    "embedder = WatsonxEmbeddings(\n",
    "    url=os.getenv(\"URL\"),\n",
    "    apikey=os.getenv(\"API_KEY\"),\n",
    "    project_id=os.getenv(\"PROJECT_ID\"),\n",
    "    model_id=\"ibm/granite-embedding-278m-multilingual\"\n",
    ")\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"climate_edu\",\n",
    "    persist_directory=\"data/chroma_db\",\n",
    "    embedding_function=embedder\n",
    ")\n",
    "\n",
    "\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.accelerator_options = AcceleratorOptions(\n",
    "    num_threads=12, device=AcceleratorDevice.AUTO\n",
    ")\n",
    "converter = DocumentConverter(\n",
    "            format_options={\n",
    "            InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)\n",
    "        })\n",
    "\n",
    "\n",
    "for file_path in file_paths:\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    file_path = os.path.join(\"data/climate_edu\", file_path)    \n",
    "    result = converter.convert(file_path)\n",
    "    doc = result.document\n",
    "    doc_name = doc.name\n",
    "    doc_md = doc.export_to_markdown(page_break_placeholder=PAGE_BREAK, image_placeholder=\"\",)\n",
    "    pages = doc_md.split(PAGE_BREAK)\n",
    "    pages = [markdown_cleanup(page) for page in pages]\n",
    "    chunked_docs = []\n",
    "    for i, page in enumerate(pages):\n",
    "        chunks = markdown_splitter.split_text(page)\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            print(f\"processing page {i + 1} of {doc_name}\")\n",
    "            document = Document(\n",
    "                page_content=chunk,\n",
    "                embeddings=tokenizer.encode(chunk, add_special_tokens=False),\n",
    "                metadata={\n",
    "                    \"file_name\": doc_name,\n",
    "                    \"page\": i + 1,\n",
    "                    \"chunk\": j + 1\n",
    "                }\n",
    "            )\n",
    "            chunked_docs.append(document)\n",
    "    print(\"done processing file: \", file_path)\n",
    "    vector_store.add_documents(chunked_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ibm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
